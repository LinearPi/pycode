{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import jieba\n",
    "import numpy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43mZhuLangXinSong-BiaoZhun\u001b[m\u001b[m     get_info.py\r\n",
      "ZhuLangXinSong-BiaoZhun.zip \u001b[31mmaster.zip\u001b[m\u001b[m\r\n",
      "analysis_message.ipynb      resewq.py\r\n",
      "analysis_message_done.ipynb \u001b[34mstopwords\u001b[m\u001b[m\r\n",
      "ask_message.csv             suggest_message.csv\r\n",
      "\u001b[31mchromedriver\u001b[m\u001b[m                train.py\r\n",
      "complain_message.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取读取\n",
    "import mongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "db = client.OPA\n",
    "all_data = db.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理，处理需要的数据\n",
    "need_data = pandas.DataFrame({\"title\":all_data.XXX,\n",
    "                              \"content\": all_data.XXX,                              \n",
    "                              \"\": ,\n",
    "                             })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印数据\n",
    "print(need_data.tail())\n",
    "# 打印数据描述\n",
    "print(need_data.describe())\n",
    "# 查看数据的维度\n",
    "print(need_data.shape)\n",
    "# 查看数据类型\n",
    "print(type(need_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理数据\n",
    "# 清理内容字符数小于6的数据\n",
    "need_data = need_data[need_data['content'].str.len() > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感分析\n",
    "\"\"\"\n",
    "在这部分内容中，我们调用了SnowNLP的情感分析，它是一个python写的类库，可以方便的处理中文文本内容，不用我们实现其中具体的代码。\n",
    "一般来说，情感分析的目的是为了找出作者观点的态度，是正向还是负向，或者更具体的，我们希望知道他的情绪。\n",
    "在这里，我们希望了解到好友签名所表达出来的情感是积极的，还是中立、负面的，\n",
    "比如说在以下例子中，我们对\"这个商品我非常喜欢，颜色很合我意！\"这句话进行了预处理，并通过训练好的模型预测其的情感。\n",
    "在这里，我们简单地假设大于0.66表示积极，低于0.33表示消极，其他表示中立。\"\"\"\n",
    "\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# #一个demo\n",
    "# text = \"这个商品我非常喜欢，颜色很合我意！\"\n",
    "# sentiment = SnowNLP(text).sentiments\n",
    "# print(sentiment)\n",
    "\n",
    "\n",
    "# 用一个列表存储经过情感分析处理的数据\n",
    "sentiments = []\n",
    "for i in tList:\n",
    "    sentiments.append(SnowNLP(i).sentiments) \n",
    "    \n",
    "# 打印数据\n",
    "print(sentiments)\n",
    "\n",
    "# 使用一个字典返回情感分析统计的数据\n",
    "sentiments_dict = {\"positive\": 0,\n",
    "                    \"neutral\": 0,\n",
    "                    \"negative\": 0}\n",
    "\n",
    "for i in sentiments:\n",
    "    if i > 0.66:\n",
    "        sentiments_dict[\"positive\"] += 1\n",
    "    elif 0.33 <= i <= 0.66:\n",
    "        sentiments_dict[\"neutral\"] += 1\n",
    "    elif 0.33 > i:\n",
    "        sentiments_dict[\"negative\"] += 1\n",
    "\n",
    "print(sentiments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  打印试试看看图片，可以用于前端展示\n",
    "labels = [u'Negative',u'Neutral',u'Positive']\n",
    "values = (negative,neutral,positive)\n",
    "plt.xlabel(u'Sentiment Analysis')\n",
    "plt.ylabel(u'Number')\n",
    "plt.xticks(range(3),labels)\n",
    "plt.bar(range(3), values)\n",
    "\n",
    "plt.title('Sentiment Analysis of OPA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转列表 列表才可以使用数据\n",
    "content = need_data.content.values.tolist()\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'领导您好： 我的新名称已预审通过，请问从哪里下载： 1.名称申请表 2.名称审查表'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "content[random.randint(1,2655)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0t/znq2g8qn0mscy1hmx_2wcdj00000gn/T/jieba.cache\n",
      "Loading model cost 1.042 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655\n"
     ]
    }
   ],
   "source": [
    "# 使用分词，把文章分为单独词语\n",
    "content_s = []\n",
    "for i in content:\n",
    "    current_mess = jieba.lcut(i)\n",
    "    # 判断词语\n",
    "    if len(current_mess) > 2 and current_mess!='\\r\\n' and current_mess != ' ': \n",
    "        content_s.append(current_mess)\n",
    "print(len(content_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content = pandas.DataFrame({\"contene_s\":content_s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655\n"
     ]
    }
   ],
   "source": [
    "# 查看分词之后的情况 随机查看\n",
    "content_s[random.randint(1,2655)]\n",
    "print(len(content_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>、</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        ?\n",
       "1        、\n",
       "2        。\n",
       "3        “\n",
       "4        ”"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取停用词，停用词的使用，是为减少一个常见词语对这个语意的影响\n",
    "stopwords = pandas.read_csv('stopwords/stop_words_zh.txt',index_col=False, sep=\"\\t\", quoting = 3, names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉停用词\n",
    "def drop_stopword(contents, stopwords):\n",
    "    content_clean = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)            \n",
    "        content_clean.append(line_clean)\n",
    "    return content_clean\n",
    "\n",
    "contents = df_content.contene_s.values.tolist()\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "content_clean =drop_stopword(contents, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[工商, 审批, 10, 多天, 没, 进展, 不是, 说好, 一到, 2, 工作日]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[甘肃, 处处, 落后, 网站, 不, 例外, 经济, 发展, 模式, 不好, 抄, 照搬,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[尊敬, 领导, 之前, 教师, 职称, 评审, 政策, 都, 班主任, 工作, 年限, 不...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[政府, 采购, 协议, 供货, 合同, 以前, 采用, 人工, 备案, 财政厅, 需要, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[甘肃, 公安, 政务, 服务平台, 浏览器, 不, 兼容, 360, 浏览器, 每次, 登...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       content_clean  label\n",
       "0        [工商, 审批, 10, 多天, 没, 进展, 不是, 说好, 一到, 2, 工作日]      1\n",
       "1  [甘肃, 处处, 落后, 网站, 不, 例外, 经济, 发展, 模式, 不好, 抄, 照搬,...      1\n",
       "2  [尊敬, 领导, 之前, 教师, 职称, 评审, 政策, 都, 班主任, 工作, 年限, 不...      1\n",
       "3  [政府, 采购, 协议, 供货, 合同, 以前, 采用, 人工, 备案, 财政厅, 需要, ...      1\n",
       "4  [甘肃, 公安, 政务, 服务平台, 浏览器, 不, 兼容, 360, 浏览器, 每次, 登...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_drop = pandas.DataFrame({\"content_clean\": content_clean, 'label':all_content['label']})\n",
    "df_content_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "企业法人注册一直显示实名认证失败，但是个人注册可以，不知道能否改进下，让企业法人注册方便一点，开一个公司不容易，好几天先注册不上，很浪费时间的，麻烦有关领导看见了能不能改进下。\n",
      "注册 企业法人 改进 不上 浪费时间\n"
     ]
    }
   ],
   "source": [
    "# 使用jieba分析语句中重要的词语，提取一段的关键词，减少阅读量   参数topK 是最后返回数据的类型\n",
    "import jieba.analyse\n",
    "index = 80\n",
    "print(sug_content['content'][index])\n",
    "content_s_str = \"\".join(content_s[index])\n",
    "print(' '.join(jieba.analyse.extract_tags(content_s_str, topK = 5, withWeight = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据用于词云使用\n",
    "words_count = df_all_drop.groupby(by=['all_words'])['all_words'].agg({\"counts\": numpy.size})\n",
    "words_count = words_count.reset_index().sort_values(by=['counts'], ascending = False)\n",
    "words_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画词云\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path= 'ZhuLangXinSong-BiaoZhun/ZhuLangXinSong-BiaoZhun-2.otf',background_color='white', max_font_size=80)\n",
    "word_frequence = {x[0]:x[1] for x in words_count.head(300).values}\n",
    "wordcloud = wordcloud.fit_words(word_frequence)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用LDA：主题模型\n",
    "格式要求：list of list，分词好的整个语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(content_clean)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类似Kmeans \n",
    "lda = gensim.models.LdaModel(corpus = corpus, id2word = dictionary, num_topics = 3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015*\"月\" + 0.014*\"办理\" + 0.013*\"年\" + 0.009*\"日\" + 0.008*\"需要\"\n"
     ]
    }
   ],
   "source": [
    "# 1号分类结果\n",
    "print(lda.print_topic(1,topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011*\"年\" + 0.010*\"不\" + 0.008*\"月\" + 0.008*\"办理\" + 0.006*\"没有\"\n",
      "0.015*\" \" + 0.009*\"办理\" + 0.009*\"申请\" + 0.008*\"需要\" + 0.007*\"没有\"\n",
      "0.016*\"办理\" + 0.012*\"公司\" + 0.011*\"不\" + 0.011*\"名称\" + 0.010*\"申请\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics = 3, num_words = 5):\n",
    "    print(topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印有多少个分类\n",
    "all_content.label.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本来的标签值就是为123，所有不需要做标签值的转变\n",
    "# # 把lable的值换成是数字\n",
    "# label_mapping = {\"建议\":1, \"咨询\":2, \"投诉\":3}\n",
    "# all_content['label'] = all_content['label'].map(label_mapping)\n",
    "# all_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2655,)\n",
      "(2655, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_content_drop['content_clean'].shape)\n",
    "print(all_content.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[工商, 审批, 10, 多天, 没, 进展, 不是, 说好, 一到, 2, 工作日]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[甘肃, 处处, 落后, 网站, 不, 例外, 经济, 发展, 模式, 不好, 抄, 照搬,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[尊敬, 领导, 之前, 教师, 职称, 评审, 政策, 都, 班主任, 工作, 年限, 不...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[政府, 采购, 协议, 供货, 合同, 以前, 采用, 人工, 备案, 财政厅, 需要, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[甘肃, 公安, 政务, 服务平台, 浏览器, 不, 兼容, 360, 浏览器, 每次, 登...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       content_clean  label\n",
       "0        [工商, 审批, 10, 多天, 没, 进展, 不是, 说好, 一到, 2, 工作日]      1\n",
       "1  [甘肃, 处处, 落后, 网站, 不, 例外, 经济, 发展, 模式, 不好, 抄, 照搬,...      1\n",
       "2  [尊敬, 领导, 之前, 教师, 职称, 评审, 政策, 都, 班主任, 工作, 年限, 不...      1\n",
       "3  [政府, 采购, 协议, 供货, 合同, 以前, 采用, 人工, 备案, 财政厅, 需要, ...      1\n",
       "4  [甘肃, 公安, 政务, 服务平台, 浏览器, 不, 兼容, 360, 浏览器, 每次, 登...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据  训练集和测试集默认分为3/1  test_size= 0~1\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_content_drop['content_clean'].values, df_content_drop['label'].values, random_state = 1, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['个体户', '名称', '申请', '审核', '都', '好', '几天', '没有', '一点', '进展', '现在', '不是', '简化', '程序', '还', '慢']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1991,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#打印训练集大小\n",
    "print(x_train[1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好 2013 年 甘肃农业大学 取得 农业 推广 硕士学位 当时 报考 在职 研究生 没有 毕业证 学位证 不 知道 报考\n"
     ]
    }
   ],
   "source": [
    "# 列表转字符串\n",
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except IndexError:\n",
    "        print(line_index, words.index)\n",
    "print(words[0])\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 0 0]]\n",
      "[3 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 测试词语转向量\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts = ['dog cat fish', 'dog cat cat', 'fish, bird', 'bird','bird cat']\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(texts)\n",
    "\n",
    "# 打印结果\n",
    "# 打印所有的词语\n",
    "print(cv.get_feature_names())\n",
    "# 打印词语占的向量\n",
    "print(cv_fit.toarray())\n",
    "# 打印合计\n",
    "print(cv_fit.toarray().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词语转向量 设置转换规则 计数\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word', max_features=5000, lowercase = False)\n",
    "vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifer = MultinomialNB()\n",
    "classifer.fit(vec.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好 政务网 企业 登录 后 需要 进行 企业 信息 变更 出现 确认 企业 信息 窗口 工商 密码 填写 告知 谢谢'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集数据处理\n",
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "        print(line_index, word_index)\n",
    "test_words[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765060240963856"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印准确率\n",
    "classifer.score(vec.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 方法2 tf词频使用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用tf词频\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features = 5000, lowercase = False)\n",
    "vectorizer.fit(words)\n",
    "# 打印结果\n",
    "# 训练\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifer = MultinomialNB()\n",
    "classifer.fit(vectorizer.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8358433734939759"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印准确率\n",
    "classifer.score(vectorizer.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 验证数据\n",
    "# perdict_data = cv.fit_transform(x_train[1])\n",
    "# perdict_data\n",
    "# classifer.predict(perdict_data)\n",
    "# classifer.predict(vec.transform(x_train[1882]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证某条数据\n",
    "new_str = '甘肃省省领导： 我公司景泰长城冶炼有限责任公司，在2016年电力用户和发电企业的直购电交易上'\n",
    "perdict_str = jieba.lcut(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义去掉停用词函数\n",
    "def drop_stopword_peridict(contents, stopwords):\n",
    "    content_clean = []\n",
    "    for word in contents:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        content_clean.append(word)            \n",
    "    return content_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉停用词\n",
    "peridict_clean =drop_stopword_peridict(perdict_str, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'咨询'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer_dict = {1: \"建议\", 2: \"咨询\",3: \"投诉\"}\n",
    "classifer_dict[classifer.predict(vec.transform(peridict_clean))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  因为咨询的数据量过大，在这里里面成为了主要的分类。 模型的正确率不是很高， 可以有两个地方提高正确率，\n",
    "# 1.数据采集，需要均衡三类数据，\n",
    "# 2.停用词，需要对具体情况进行，因为时间的原因，我在网上找了很多的停用词来使用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
